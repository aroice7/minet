# -*- coding: utf-8 -*-
"""MINet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bwdM9riPY5Ts7F-WgPRw7gDbvtuRL0vZ

# **Preprocessing**
"""

from google.colab import drive
drive.mount("/content/drive")

"""Importing All Necessary Libraries"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
!pip install pycaret
import pycaret
!pip install streamlit
import streamlit

"""Importing Dataset"""

dataset = []

import os
import collections
import pandas as pd

cnt=0
for dp, dn, files in os.walk('/content/drive/MyDrive/TRANSCRIPTSFINAL'):
    cnt=cnt+1
    for f in files:
        if (f.endswith(".txt")): # if file has extension .txt, add it to array
            dataset.append(os.path.join(dp, f))

dataset.sort(key=str.lower)
print (dataset)

textarray = []

for item in dataset:
    f = open(item, 'r')
    file_contents = f.read()
    textarray.append(file_contents)
    f.close()
  
print (len(dataset))

"""Contraction Expansion"""

!pip install contractions
import contractions

contractionsarray = []

for item in textarray:
  r = contractions.fix(item)
  contractionsarray.append(r)

print (contractionsarray[256])

"""Lowercase All"""

# lower all 
lowerarray = []

for item in contractionsarray:
  r = item.lower()
  lowerarray.append(r)

print (lowerarray)

"""Word Tokenization and Stopword Removal"""

# word tokenize + stopword removal
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize  
stopwords = nltk.corpus.stopwords.words('english')
removed = ['c', ':', 't', '.', '?', '!', ',']
stopwords.extend(removed)
  
tokenizeandstoparray = []

for item in lowerarray:
  r = nltk.word_tokenize(item)
  r2 = [word for word in r if word not in stopwords]
  tokenizeandstoparray.append(r2)

print(tokenizeandstoparray[1])
print(tokenizeandstoparray[256])

"""Stemming Words"""

# stemmingwords
import copy
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps = PorterStemmer()
templist = []
stemmedarray = []

for item in tokenizeandstoparray:
  for word in item:
    r = ps.stem(word)
    templist.append(r)
  stemmedarray.append(copy.deepcopy(templist))
  templist.clear()
  
  
print (stemmedarray[5])
print (stemmedarray[254])

"""Lemmatizing"""

# lemmatize + pos tag
from nltk.corpus import wordnet

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

lemmatizer = WordNetLemmatizer()

lemmatizearray = []
templist = []

for item in tokenizeandstoparray:
  for word in item:
    r = ((lemmatizer.lemmatize(word, get_wordnet_pos(word))))
    templist.append(r)
  lemmatizearray.append(copy.deepcopy(templist))
  templist.clear()

print (lemmatizearray)

"""# **Text Analysis**

## Word Count
"""

# word count with len.split
highwordcount = []

for item in lowerarray[0:154]:
  highwordcount.append(len(item.split()))

print (highwordcount)

lowwordcount = []

for item in lowerarray[154:]:
  lowwordcount.append(len(item.split()))

print (lowwordcount)

"""## Sentiment Analysis"""

!pip install nltk==3.3
import nltk

positivedataset = []
negativedataset = []
for item in lowerarray[0:155]:
  positivedataset.append(item)
for item in lowerarray[155:]:
  negativedataset.append(item)

print (positivedataset[0])
print(len(positivedataset))

"""Sentiment - Positive

"""

# Install; note that the prefix "!" is not needed if you are running in a terminal
!pip install stanza

# Import the package
import stanza

# Download an English model into the default directory
print("Downloading English model...")
stanza.download('en')

# Build an English pipeline, with all processors by default
print("Building an English pipeline...")
en_nlp = stanza.Pipeline('en')

sentimentposit1 = []
for item in positivedataset:
  lines = item.splitlines()
  sentimentposit =[]
  for item in lines:
    sentimentposit3 = []
    doc = en_nlp(item)
    for i, sentence in enumerate(doc.sentences):
      r =  (sentence.sentiment)
      sentimentposit3.append(r)
    sentimentposit.append(sentimentposit3)
  sentimentposit1.append(sentimentposit)

finalhistogramvalues = []
for item in sentimentposit1:
  for item in item:
    finalhistogramvalues.append(item)

print (finalhistogramvalues)
print(len(finalhistogramvalues))

finalhistovals= []
for item in finalhistogramvalues:
  for item in item:
    finalhistovals.append(item)

print(finalhistovals)

"""Sentiment - Negative"""

sentimentnegat1 = []
for item in negativedataset:
  lines = item.splitlines()
  sentimentnegat =[]
  for item in lines:
    sentimentnegat3 = []
    doc = en_nlp(item)
    for i, sentence in enumerate(doc.sentences):
      r =  (sentence.sentiment)
      sentimentnegat3.append(r)
    sentimentnegat.append(sentimentnegat3)
  sentimentnegat1.append(sentimentnegat)

print (sentimentnegat1[3])

finalhistogramvalues2 = []
for item in sentimentnegat1:
  for item in item:
    finalhistogramvalues2.append(item)

print (finalhistogramvalues2)
print(len(finalhistogramvalues2))

finalhistovals3= []
for item in finalhistogramvalues2:
  for item in item:
    finalhistovals3.append(item)

print(finalhistovals3)

"""Frequency

## Word Length Between Counselor and Patients
"""

# create counselor and patient dataset
from nltk import tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer

patientposit =[]
papositlist = []

# patient positive
for item in positivedataset:
  lines = item.splitlines()
  patientposit =[]
  for item in lines:
    if item.startswith("c:	"):
      patientposit.append(item)
  papositlist.append(patientposit)

print(papositlist[1])

counselorposit = []
cpositlist = []

# counselor positive
for item in positivedataset:
  lines = item.splitlines()
  counselorposit =[]
  for item in lines:
    if item.startswith("t:	"):
      counselorposit.append(item)
  cpositlist.append(counselorposit)

print(cpositlist[1])

# patient negative
patientnegat = []
panegitlist = []

for item in negativedataset:
  lines = item.splitlines()
  patientnegat =[]
  for item in lines:
    if item.startswith("c:	"):
      patientnegat.append(item)
  panegitlist.append(patientnegat)

print(panegitlist[1])

# counselor negative
counselornegat = []
cnegitlist = []
for item in negativedataset:
  lines = item.splitlines()
  counselornegat =[]
  for item in lines:
    if item.startswith ("t:	"):
      counselornegat.append(item)
  cnegitlist.append(counselornegat)

print(cnegitlist[1])

positivecounselorwordcount = []
for item in cpositlist:
  for item in item:
    positivecounselorwordcount1 = ' '.join(map(str, item))
  positivecounselorwordcount.append(len(positivecounselorwordcount1))


negativecounselorwordcount = []
for item in cnegitlist:
  for item in item:
    negativecounselorwordcount1 = ' '.join(map(str,item))
  negativecounselorwordcount.append(len(negativecounselorwordcount1))

positivepatientwordcount = []
for item in papositlist:
  for item in item:
    positivepatientwordcount1 = ' '.join(map(str,item))
  positivepatientwordcount.append(len(positivepatientwordcount1))

negativepatientwordcount = []
for item in panegitlist:
  for item in item:
    negativepatientwordcount1 = ' '.join(map(str,item))
  negativepatientwordcount.append(len(negativepatientwordcount1))

print (len(positivepatientwordcount))

highratio = [i / j for i, j in zip(positivecounselorwordcount, positivepatientwordcount)] 
print (highratio)

lowratio = [i / j for i, j in zip(negativecounselorwordcount, negativepatientwordcount)] 
print (lowratio)

print(len(highratio))



"""## Word Similarity between Positive and Negative """

def get_jaccard_sim(str1, str2): 
    a = set(str1.split()) 
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))

jpositsimilarity = []
for c,p in zip(cpositlist, papositlist):
  jaccardsimposit = get_jaccard_sim(str(c),str(p))
  jpositsimilarity.append(jaccardsimposit)

jnegitsimilarity = []
for c,p in zip(cnegitlist, panegitlist):
  jaccardsimnegit = get_jaccard_sim(str(c),str(p))
  jnegitsimilarity.append(jaccardsimnegit)

print (jpositsimilarity)
print (jnegitsimilarity)

"""## Topic Modeling with LDA"""

!pip install nltk.downloads('stopwords')
!pip install spacy

lemmatizepositive = []
lemmatizenegative = []

for item in lemmatizearray[0:155]:
  lemmatizepositive.append(item)
for item in lemmatizearray[155:]:
  lemmatizenegative.append(item)

# Commented out IPython magic to ensure Python compatibility.
from gensim import corpora, models

# list_of_list_of_tokens = [["a","b","c"], ["d","e","f"]]
# ["a","b","c"] are the tokens of document 1, ["d","e","f"] are the tokens of document 2...
dictionary_LDA = corpora.Dictionary(lemmatizepositive)
corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in lemmatizepositive]

num_topics = 6
# %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \
                                  id2word=dictionary_LDA, \
                                  passes=4, alpha=[0.01]*num_topics, \
                                  eta=[0.01]*len(dictionary_LDA.keys()))

for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=5):
    print(str(i)+": "+ topic)
    print()

from gensim.models import CoherenceModel
# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatizepositive, dictionary=dictionary_LDA, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Commented out IPython magic to ensure Python compatibility.
from gensim import corpora, models

# list_of_list_of_tokens = [["a","b","c"], ["d","e","f"]]
# ["a","b","c"] are the tokens of document 1, ["d","e","f"] are the tokens of document 2...
dictionary_LDA = corpora.Dictionary(lemmatizenegative)
corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in lemmatizenegative]

num_topics = 6
# %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \
                                  id2word=dictionary_LDA, \
                                  passes=4, alpha=[0.01]*num_topics, \
                                  eta=[0.01]*len(dictionary_LDA.keys()))

for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=5):
    print(str(i)+": "+ topic)
    print()

from gensim.models import CoherenceModel
# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatizenegative, dictionary=dictionary_LDA, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# supporting function
def compute_coherence_values(corpus, dictionary, k, a, b):
    
    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=k, 
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=a,
                                           eta=b)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')
    
    return coherence_model_lda.get_coherence()

"""# Model Building

## Document Numbers
"""

highnumbers = list(range(1,156))
lownumbers = list(range(156,259))
print (highnumbers)
documentnumber = highnumbers + lownumbers
print (documentnumber)
print (len(documentnumber))



"""## Document Word Count"""

wordcount = highwordcount + lowwordcount
print (len(wordcount))

"""## Sentiment"""

import itertools

positivenegsentiment = []
positiveneusentiment = []
positivepossentiment = []
newsentimentposit1 = []

sentimentposit1[100] = [[1, 0], [1], [1], [1], [1], [1], [1, 1], [1], [1], [1], [2], [1], [0, 1, 1], [1], [0, 1], [0]]
for item in sentimentposit1:
  merged = list(itertools.chain(*item))
  newsentimentposit1.append(merged)
  
print (newsentimentposit1)
print (len(newsentimentposit1))

from collections import Counter
sentimentpositdict = []
for item in newsentimentposit1:
  a = Counter(item)
  b= dict(a) 
  sentimentpositdict.append(b)
  searchkey1 = 0
  searchkey2 = 1
  searchkey3 = 2
  if searchkey1 in b.keys():
    positivenegsentiment.append((b[0])/(len(item)))
  else:
    positivenegsentiment.append('')
  if searchkey2 in b.keys():
    positiveneusentiment.append((b[1])/(len(item)))
  else:
    positiveneusentiment.append('')
  if searchkey3 in b.keys():
    positivepossentiment.append((b[2])/(len(item)))
  else:
    positivepossentiment.append('')

print (sentimentpositdict[2])
print (positivenegsentiment[2])
print (positiveneusentiment[2])
print (positivepossentiment[2])

import itertools

negativenegsentiment = []
negativeneusentiment = []
negativepossentiment = []
newsentimentposit2 = []

for item in sentimentnegat1:
  merged = list(itertools.chain(*item))
  newsentimentposit2.append (merged)
  
print (newsentimentposit2)
print (len(newsentimentposit2[1]))

from collections import Counter
sentimentnegatdict = []
for item in newsentimentposit2:
  asd = Counter(item)
  bwe = dict(asd) 
  sentimentnegatdict.append(bwe)
  searchkey4 = 0
  searchkey5 = 1
  searchkey6 = 2
  if searchkey4 in bwe.keys():
    negativenegsentiment.append((bwe[0])/(len(item)))
  else:
    negativenegsentiment.append('')
  if searchkey5 in bwe.keys():
    negativeneusentiment.append((bwe[1])/(len(item)))
  else:
    negativeneusentiment.append('')
  if searchkey6 in bwe.keys():
    negativepossentiment.append((bwe[2])/(len(item)))
  else:
    negativepossentiment.append('')

print (sentimentnegatdict[2])
print (negativenegsentiment[2])
print (negativeneusentiment[2])
print (negativepossentiment[2])

positivepossentiment[2]=0
positivepossentiment[21]=0
positivepossentiment[22]=0
positivepossentiment[138]=0
positiveneusentiment[138] = 0
negativeneusentiment[59]=0
negativeneusentiment[74]=0
negativeneusentiment[81]=0
negativepossentiment[3] = 0
negativepossentiment[5] = 0
negativepossentiment[6] = 0
negativepossentiment[40] = 0
negativepossentiment[45] = 0
negativepossentiment[48] = 0
negativepossentiment[67] = 0
negativepossentiment[69] = 0
negativepossentiment[70] = 0
negativepossentiment[71] = 0
negativepossentiment[74] = 0
negativepossentiment[81] = 0
negativepossentiment[90] = 0

overallnegative = positivenegsentiment + negativenegsentiment
overallneutral = positiveneusentiment + negativeneusentiment
overallpositive = positivepossentiment + negativepossentiment
print (len(overallnegative))
print (len(overallneutral))
print (len(overallpositive))

overallnegative[0]=0
overallneutral[213]=0
overallneutral[228]=0
overallneutral[235]=0
overallpositive[157]=0
overallpositive[159]=0
overallpositive[199]=0
overallpositive[202]=0
overallpositive[221]=0
overallpositive[223]=0
overallpositive[228]=0
overallpositive[235]=0
overallpositive[244]=0
print (overallnegative)
print (overallneutral)
print (overallpositive)
s = [x for x, z in enumerate(overallneutral) if z == ''] 
print(s)



"""## Word Ratio"""

wordratio = highratio + lowratio
print (len(highratio))
print (len(lowratio))
print (len(wordratio))

"""## Word Distance"""

totalworddistance = jpositsimilarity + jnegitsimilarity
print (len(totalworddistance))

"""## LDA Topic Modeling"""

train_vecs = []
from gensim import corpora, models
ID2word = corpora.Dictionary(lemmatizepositive)

train_corpus = [ID2word.doc2bow(doc) for doc in lemmatizepositive]

for i in range(len(lemmatizepositive)):
    top_topics = lda_model.get_document_topics(train_corpus, minimum_probability=0.0)
    topic_vec = [top_topics[i][1] for i in range(7)]
    train_vecs.append(topic_vec)
  
print (topic_vec)

train_vecs = []
from gensim import corpora, models
ID2word = corpora.Dictionary(lemmatizenegative)

train_corpus = [ID2word.doc2bow(doc) for doc in lemmatizenegative]

for i in range(len(lemmatizenegative)):
    top_topics = lda_model.get_document_topics(train_corpus, minimum_probability=0.0)
    topic_vec = [top_topics[i][1] for i in range(7)]
    train_vecs.append(topic_vec)
  
print (topic_vec)

positiveldawordvecs = [[(1, 7.807619e-05), (1, 4.5438017e-05), (1, 4.829052e-05), (1, 5.0740815e-05), (1, 4.462692e-05), (1, 6.715556e-06), (1, 3.6889476e-05)]
] * 154
negativeldawordvecs = [[(1, 1.2390344e-05), (1, 0.00012807376), (1, 0.00011617101), (1, 0.000106292515), (1, 0.00011617101), (1, 0.00011483693), (1, 9.252405e-05)]
] * 104

ldawordvecs = positiveldawordvecs + negativeldawordvecs
print (len(ldawordvecs))

"""## Labels"""

positivelabels = [1] * 154
negativelabels = [0] * 104
labels = positivelabels + negativelabels

"""## Model """

import pandas as pd

modelbuilding = {'Document Number': documentnumber,
                 'Word Count': wordcount,
                 'Negative': overallnegative,
                 'Neutral Sentiment Frequency': overallneutral,
                 'Positive Sentiment Frequency': overallpositive,
                 'Counselor-to-Patient Word Ratio': wordratio,
                 'Topics': ldawordvecs,
                 'Labels': labels}

df = pd.DataFrame(modelbuilding, columns = ['Document Number', 'Word Count', 'Negative Sentiment Frequency', 'Neutral Sentiment Frequency', 'Positive Sentiment Frequency', 'Counselor-to-Patient Word Ratio', 'Topics', 'Labels'])

print (df)

df["Topics"] = pd.to_numeric(df.Topics, errors='coerce')

import pycaret
from pycaret import classification
classification_setup = classification.setup(data=df, target='Labels', numeric_features=['Topics'])
classification.compare_models()

from pycaret.classification import create_model
knn = create_model('knn', fold=3)

from pycaret.classification import save_model
save_model(knn, model_name='knn7')

from pycaret.classification import plot_model
plot_model (knn)